{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2621656-91eb-4a8a-9288-b47556f5a5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchmetrics in ./.local/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /cluster/apps/nss/jupyterhub/3.5.1/lib64/python3.10/site-packages (from torchmetrics) (1.25.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in ./.local/lib/python3.10/site-packages (from torchmetrics) (0.9.0)\n",
      "Requirement already satisfied: torch>=1.8.1 in /cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/lib64/python3.10/site-packages (from torchmetrics) (1.11.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in /cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/lib64/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.2.0)\n",
      "Requirement already satisfied: packaging>=17.1 in /cluster/apps/nss/jupyterhub/3.5.1/lib64/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b2e767-a7c0-4656-b223-450edb0c3301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fontconfig warning: ignoring C.UTF-8: not a valid language tag\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchmetrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19efe23-447e-4e41-bb71-9aa043149fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp.30552059.ewybitul/ipykernel_1854/1032208847.py:4: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "data_path = '~/data/all_DEFR_comments_27062022.csv'\n",
    "\n",
    "# , dtype={'ID':float, 'kommentar_original':str, 'label':int}\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ab7492-9f55-4ae8-a1b5-f258fadebf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleID</th>\n",
       "      <th>ID</th>\n",
       "      <th>geschlecht</th>\n",
       "      <th>alter</th>\n",
       "      <th>sexualitaet</th>\n",
       "      <th>religion</th>\n",
       "      <th>nationalitaet</th>\n",
       "      <th>beeintraechtigung</th>\n",
       "      <th>sozialer_status</th>\n",
       "      <th>politik</th>\n",
       "      <th>aussehen</th>\n",
       "      <th>andere</th>\n",
       "      <th>toxische_sprache</th>\n",
       "      <th>label</th>\n",
       "      <th>kommentar_original</th>\n",
       "      <th>kommentar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>395000000000000000.0</td>\n",
       "      <td>977000000000000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ich denke sie sollten sich erhängen. Wo haben ...</td>\n",
       "      <td>ich denke sie sollten sich erhängen. wo haben ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>479000000000000000.0</td>\n",
       "      <td>1010000000000000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Macjst du Trolle aus normale Menschen machen, ...</td>\n",
       "      <td>macjst du trolle aus normale menschen machen, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>490000000000000000.0</td>\n",
       "      <td>1556419213.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Christentum.hat weder Sharia noch Koran , die ...</td>\n",
       "      <td>christentum.hat weder sharia noch koran , die ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ArticleID                     ID  geschlecht  alter  \\\n",
       "0  395000000000000000.0   977000000000000000.0           0    0.0   \n",
       "1  479000000000000000.0  1010000000000000000.0           0    0.0   \n",
       "2  490000000000000000.0           1556419213.0           0    0.0   \n",
       "\n",
       "   sexualitaet  religion  nationalitaet  beeintraechtigung  sozialer_status  \\\n",
       "0            0         0              0                0.0                0   \n",
       "1            0         0              0                0.0                0   \n",
       "2            0         1              0                0.0                0   \n",
       "\n",
       "   politik  aussehen  andere  toxische_sprache  label  \\\n",
       "0        0         0       1                 0      1   \n",
       "1        0         0       0                 0      0   \n",
       "2        0         0       0                 0      1   \n",
       "\n",
       "                                  kommentar_original  \\\n",
       "0  Ich denke sie sollten sich erhängen. Wo haben ...   \n",
       "1  Macjst du Trolle aus normale Menschen machen, ...   \n",
       "2  Christentum.hat weder Sharia noch Koran , die ...   \n",
       "\n",
       "                                           kommentar  \n",
       "0  ich denke sie sollten sich erhängen. wo haben ...  \n",
       "1  macjst du trolle aus normale menschen machen, ...  \n",
       "2  christentum.hat weder sharia noch koran , die ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df07e3b7-69ea-4500-b30c-c261d17a4626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at statworx/bert-base-german-cased-finetuned-swiss were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at statworx/bert-base-german-cased-finetuned-swiss and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"statworx/bert-base-german-cased-finetuned-swiss\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"statworx/bert-base-german-cased-finetuned-swiss\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85dbf393-3d10-40ab-80a5-fd57c6189bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[:-1000]\n",
    "test = df[-1000:]\n",
    "train, val = train_test_split(df_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "train_X, train_y = train[\"kommentar\"], train[\"label\"]\n",
    "test_X, test_y = test[\"kommentar\"], test[\"label\"]\n",
    "val_X, val_y = val[\"kommentar\"], val[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16e80da3-4049-4c7d-a417-67ed13078a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatchedDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data.iloc[index]\n",
    "        y = torch.tensor(self.labels.iloc[index], dtype = torch.float32)\n",
    "        return x, y\n",
    "\n",
    "train_dataset = MyBatchedDataset(train_X[:1000], train_y[:1000])\n",
    "val_dataset = MyBatchedDataset(val_X, val_y)\n",
    "test_dataset = MyBatchedDataset(test_X, test_y)\n",
    "batch_size = 16\n",
    "val_batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8641fb3-7b8a-48e6-beb6-ded943e715a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batched\n",
    "import math\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"statworx/bert-base-german-cased-finetuned-swiss\")\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"statworx/bert-base-german-cased-finetuned-swiss\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-german\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-german\", num_labels = 2)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-german-cased-hatespeech-GermEval18Coarse\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"deepset/bert-base-german-cased-hatespeech-GermEval18Coarse\", num_labels=2)\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"german-nlp-group/electra-base-german-uncased\")\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"german-nlp-group/electra-base-german-uncased\")\n",
    "\n",
    "#NUM_CLASSES = 2\n",
    "#model.classifier = torch.nn.Linear(in_features=model.classifier.in_features, out_features=NUM_CLASSES)\n",
    "\n",
    "#criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "# Freeze specific sublayers\n",
    "freeze_sublayers = ['encoder.layer.0.', 'encoder.layer.1.', 'encoder.layer.2.',\n",
    "                    'encoder.layer.3.', 'encoder.layer.4.', 'encoder.layer.5.',\n",
    "                    'encoder.layer.6.', 'encoder.layer.7.', 'encoder.layer.8.',\n",
    "                    'encoder.layer.9.', 'encoder.layer.10.']\n",
    "for name, param in model.named_parameters():\n",
    "    for freeze_layer in freeze_sublayers:\n",
    "        if freeze_layer in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "dif_learning_rate = [3e-4, 7e-5]\n",
    "#optimizer = torch.optim.SGD([{'params': model.bert.encoder.parameters(), 'lr': dif_learning_rate[1]},\n",
    "#                                             {'params': model.classifier.parameters(), 'lr': dif_learning_rate[0]}], momentum=0.9)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c37e8905-e430-4348-96f8-48e3e8b0eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc96d40b-5d08-466a-bd10-3185334b2718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 13/63 [00:01<00:06,  8.09it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      9\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_Y \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(train_loader): \n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.999\u001b[39m:\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/lib64/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/lib64/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/lib64/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/lib64/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/lib64/python3.10/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    169\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/lib64/python3.10/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    169\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/lib64/python3.10/site-packages/torch/utils/data/_utils/collate.py:150\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mas_tensor(batch)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mfloat\u001b[39m):\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(batch)\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not str"
     ]
    }
   ],
   "source": [
    "loss = None\n",
    "epochs = 3\n",
    "best_val_loss = math.inf\n",
    "iter_no_change = 0\n",
    "\n",
    "model.train()\n",
    "\n",
    "for idx in range(epochs):\n",
    "    losses = 0\n",
    "    \n",
    "    for batch_X, batch_Y in tqdm.tqdm(train_loader): \n",
    "        if random.random() < 0.999:\n",
    "            pass\n",
    "        batch_X = [str(x) for x in batch_X]\n",
    "        batch_X = tokenizer.batch_encode_plus(batch_X, padding=True, truncation=True, return_tensors='pt')['input_ids']\n",
    "        batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "\n",
    "        logits = outputs.logits.squeeze()\n",
    "        batch_Y = torch.squeeze(batch_Y).long()\n",
    "        loss = criterion(logits, batch_Y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    print(f\"Train loss after {idx: 04d} / {epochs: 04d} epochs: {losses/len(train_loader): 0.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        targets, outs = [], []\n",
    "        for valid_X, valid_Y in val_loader:\n",
    "            valid_X = [str(x) for x in batch_X]\n",
    "            valid_X = tokenizer.batch_encode_plus(valid_X, padding=True, truncation=True, return_tensors='pt')['input_ids']\n",
    "            valid_X, valid_Y = valid_X.to(device), valid_Y.to(device)\n",
    "\n",
    "            outputs = model(valid_X)\n",
    "            print(outputs)\n",
    "            logits = outputs.logits\n",
    "            # logits = torch.squeeze(logits)\n",
    "\n",
    "            # outs.extend([logits > 0])\n",
    "            # targets.extend(valid_Y)\n",
    "            # valid_Y = valid_Y.unsqueeze(0)\n",
    "            # valid_Y = valid_Y[0, :]\n",
    "            # valid_Y = torch.squeeze(valid_Y).long()\n",
    "            val_loss += criterion(logits, valid_Y).item()\n",
    "            \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"  Val loss after {idx: 04d} / {epochs: 04d} epochs: {val_loss: 0.4f}\")\n",
    "\n",
    "        f1 = BinaryF1Score()\n",
    "        #outs_t, targets_t = torch.cat(outs, dim=0).long(), torch.cat(targets, dim=0).long()\n",
    "\n",
    "        #print(targets_t.shape)\n",
    "        #print(outs_t.shape)\n",
    "\n",
    "        #print(f\"  Val F1 after {idx: 04d} / {epochs: 04d} epochs: {f1(outs_t, targets_t): 0.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save(model, f'models/swiss_bert_model_{val_loss}.pt')\n",
    "            print(\"model saved!\")\n",
    "#                     torch.save(classifier, 'ger_classifier.pt')\n",
    "            best_val_loss = val_loss\n",
    "            iter_no_change = 0\n",
    "        if val_loss >= best_val_loss:\n",
    "            iter_no_change += 1\n",
    "            #if iter_no_change > 10:\n",
    "                #print(f\"Early stopping: val loss didn't improve for {10*100*idx} steps!\")\n",
    "                #break\n",
    "    #if iter_no_change > 10:\n",
    "        #break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep 27 2021, 10:10:37) \n[GCC 8.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1627ed696ee43d9ad3cc651e0f15b8aca98fa9e6825d6f014f426c233038bfd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
